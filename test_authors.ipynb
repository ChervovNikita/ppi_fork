{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc0c34b2-6aef-4782-be2d-0a68972e3841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(97316, 7)\n",
      "Size is : \n",
      "97316\n",
      "Length\n",
      "19463\n",
      "4866\n",
      "GCNN Loaded\n",
      "GCNN(\n",
      "  (pro1_conv1): GCNConv(1024, 1024)\n",
      "  (pro1_fc1): Linear(in_features=1024, out_features=128, bias=True)\n",
      "  (pro2_conv1): GCNConv(1024, 1024)\n",
      "  (pro2_fc1): Linear(in_features=1024, out_features=128, bias=True)\n",
      "  (relu): LeakyReLU(negative_slope=0.01)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (sigmoid): Sigmoid()\n",
      "  (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "  (out): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "AttGNN Loaded\n",
      "AttGNN(\n",
      "  (pro1_conv1): GATConv(1024, 128, heads=1)\n",
      "  (pro1_fc1): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (pro2_conv1): GATConv(1024, 128, heads=1)\n",
      "  (pro2_fc1): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (relu): LeakyReLU(negative_slope=0.01)\n",
      "  (sigmoid): Sigmoid()\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "  (out): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from metrics import *\n",
    "from data_prepare import testloader\n",
    "from models import GCNN, AttGNN\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08c3269b-5187-464c-a0aa-e4a26ad9d089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCNN Loaded\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.cuda(\"cpu\")\n",
    "model = GCNN()\n",
    "model.load_state_dict(torch.load(\"../human_features/GCN_50.pth\")) #path to load the model\n",
    "model.to(device)\n",
    "model.eval()\n",
    "predictions = torch.Tensor()\n",
    "labels = torch.Tensor()\n",
    "with torch.no_grad():\n",
    "    for prot_1, prot_2, label in testloader:\n",
    "      prot_1 = prot_1.to(device)\n",
    "      prot_2 = prot_2.to(device)\n",
    "      #print(\"H\")\n",
    "      #print(torch.Tensor.size(prot_1.x), torch.Tensor.size(prot_2.x))\n",
    "      output = model(prot_1, prot_2)\n",
    "      predictions = torch.cat((predictions, output.cpu()), 0)\n",
    "      labels = torch.cat((labels, label.view(-1,1).cpu()), 0)\n",
    "labels = labels.numpy().flatten()\n",
    "predictions = predictions.numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36d7c3b5-8e4f-446f-92ee-c3007e51f9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def choose_optimal_threshold(actual, predicted):\n",
    "    \"\"\"\n",
    "    Choose the optimal threshold that maximizes the F1 score\n",
    "    \n",
    "    Args:\n",
    "        actual (np.array): True labels\n",
    "        predicted (np.array): Predicted probabilities\n",
    "    \n",
    "    Returns:\n",
    "        float: Optimal threshold\n",
    "    \"\"\"\n",
    "    thresholds = np.linspace(0, 1, 100)\n",
    "    f1_scores = []\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        binary_actual = (actual >= np.median(actual)).astype(int)\n",
    "        binary_pred = (predicted >= threshold).astype(int)\n",
    "        \n",
    "        # Compute confusion matrix\n",
    "        tn, fp, fn, tp = confusion_matrix(binary_actual, binary_pred).ravel()\n",
    "        \n",
    "        # Compute F1 score\n",
    "        if tp + fp == 0 or tp + fn == 0:\n",
    "            f1 = 0\n",
    "        else:\n",
    "            precision = tp / (tp + fp)\n",
    "            recall = tp / (tp + fn)\n",
    "            f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "        \n",
    "        f1_scores.append(f1)\n",
    "    \n",
    "    # Return threshold with max F1 score\n",
    "    return thresholds[np.argmax(f1_scores)]\n",
    "\n",
    "def get_binary_metrics(actual, predicted, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Compute binary classification metrics for continuous labels\n",
    "    \n",
    "    Args:\n",
    "        actual (np.array): True labels\n",
    "        predicted (np.array): Predicted probabilities\n",
    "        threshold (float, optional): Classification threshold. If None, optimal threshold is computed.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Metrics including accuracy, precision, recall, F1, etc.\n",
    "    \"\"\"\n",
    "    # If no threshold provided, find optimal threshold\n",
    "    if threshold is None:\n",
    "        threshold = choose_optimal_threshold(actual, predicted)\n",
    "    \n",
    "    # Convert to binary classification\n",
    "    binary_actual = (actual >= np.median(actual)).astype(int)\n",
    "    binary_pred = (predicted >= threshold).astype(int)\n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(binary_actual, binary_pred).ravel()\n",
    "    \n",
    "    # Compute metrics\n",
    "    metrics = {\n",
    "        'threshold': threshold,\n",
    "        'accuracy': (tp + tn) / (tp + tn + fp + fn),\n",
    "        'precision': tp / (tp + fp) if tp + fp > 0 else 0,\n",
    "        'recall': tp / (tp + fn) if tp + fn > 0 else 0,\n",
    "        'specificity': tn / (tn + fp) if tn + fp > 0 else 0,\n",
    "        'f1_score': 2 * tp / (2 * tp + fp + fn) if tp > 0 else 0,\n",
    "        'auroc': roc_auc_score(binary_actual, predicted),\n",
    "        'auprc': average_precision_score(binary_actual, predicted)\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Modify existing metrics to use the new approach\n",
    "def get_mse(actual, predicted):\n",
    "    return ((actual - predicted) ** 2).mean()\n",
    "\n",
    "def get_accuracy(actual, predicted, threshold=None):\n",
    "    metrics = get_binary_metrics(actual, predicted, threshold)\n",
    "    return metrics['accuracy'] * 100.0\n",
    "\n",
    "def precision(actual, predicted, threshold=None):\n",
    "    metrics = get_binary_metrics(actual, predicted, threshold)\n",
    "    return metrics['precision']\n",
    "\n",
    "def sensitivity(actual, predicted, threshold=None):\n",
    "    metrics = get_binary_metrics(actual, predicted, threshold)\n",
    "    return metrics['recall']\n",
    "\n",
    "def specificity(actual, predicted, threshold=None):\n",
    "    metrics = get_binary_metrics(actual, predicted, threshold)\n",
    "    return metrics['specificity']\n",
    "\n",
    "def f_score(actual, predicted, threshold=None):\n",
    "    metrics = get_binary_metrics(actual, predicted, threshold)\n",
    "    return metrics['f1_score']\n",
    "\n",
    "def mcc(actual, predicted, threshold=None):\n",
    "    metrics = get_binary_metrics(actual, predicted, threshold)\n",
    "    \n",
    "    # Matthews Correlation Coefficient calculation\n",
    "    tp = metrics['precision'] * len(actual)\n",
    "    tn = metrics['specificity'] * len(actual)\n",
    "    fp = tp / metrics['precision'] - tp\n",
    "    fn = tp / metrics['recall'] - tp\n",
    "    \n",
    "    numerator = (tp * tn) - (fp * fn)\n",
    "    denominator = np.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
    "    \n",
    "    return numerator / denominator if denominator != 0 else 0\n",
    "\n",
    "def auroc(actual, predicted):\n",
    "    return get_binary_metrics(actual, predicted)['auroc']\n",
    "\n",
    "def auprc(actual, predicted):\n",
    "    return get_binary_metrics(actual, predicted)['auprc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8ae35ea-464f-487e-a701-558574511bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : 0.01565999181152305\n",
      "Accuracy : 30.707973695026713\n",
      "precision: 0.9301470588235294\n",
      "Sensititvity :0.14602706342589625\n",
      "specificity : 0.9558253681219323\n",
      "f-score : 0.25242503187184745\n",
      "MCC : 0.0787584535952667\n",
      "AUROC: 0.7492833848650995\n",
      "AUPRC: 0.9020276008243605\n"
     ]
    }
   ],
   "source": [
    "loss = get_mse(labels, predictions)\n",
    "acc = get_accuracy(labels, predictions, 0.5)\n",
    "prec = precision(labels, predictions, 0.5)\n",
    "sensitivity = sensitivity(labels, predictions,  0.5)\n",
    "specificity = specificity(labels, predictions, 0.5)\n",
    "f1 = f_score(labels, predictions, 0.5)\n",
    "mcc = mcc(labels, predictions,  0.5)\n",
    "auroc = auroc(labels, predictions)\n",
    "auprc = auprc(labels, predictions)\n",
    "\n",
    "\n",
    "print(f'loss : {loss}')\n",
    "print(f'Accuracy : {acc}')\n",
    "print(f'precision: {prec}')\n",
    "print(f'Sensititvity :{sensitivity}')\n",
    "print(f'specificity : {specificity}')\n",
    "print(f'f-score : {f1}')\n",
    "print(f'MCC : {mcc}')\n",
    "print(f'AUROC: {auroc}')\n",
    "print(f'AUPRC: {auprc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c78e4d-4fd7-455d-ab2e-0383a5f3dd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "## model with 5 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4003aea-57a6-4e45-9121-ff5dd48d51f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCNN Loaded\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.cuda(\"cpu\")\n",
    "model = GCNN()\n",
    "model.load_state_dict(torch.load(\"../human_features/GCN_5epochs.pth\")) #path to load the model\n",
    "model.to(device)\n",
    "model.eval()\n",
    "predictions = torch.Tensor()\n",
    "labels = torch.Tensor()\n",
    "with torch.no_grad():\n",
    "    for prot_1, prot_2, label in testloader:\n",
    "      prot_1 = prot_1.to(device)\n",
    "      prot_2 = prot_2.to(device)\n",
    "      #print(\"H\")\n",
    "      #print(torch.Tensor.size(prot_1.x), torch.Tensor.size(prot_2.x))\n",
    "      output = model(prot_1, prot_2)\n",
    "      predictions = torch.cat((predictions, output.cpu()), 0)\n",
    "      labels = torch.cat((labels, label.view(-1,1).cpu()), 0)\n",
    "labels = labels.numpy().flatten()\n",
    "predictions = predictions.numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c0f9845-f8cd-4acd-9406-12c7662e3db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## model 5 epochs \n",
    "\n",
    "def choose_optimal_threshold(actual, predicted):\n",
    "    \"\"\"\n",
    "    Choose the optimal threshold that maximizes the F1 score\n",
    "    \n",
    "    Args:\n",
    "        actual (np.array): True labels\n",
    "        predicted (np.array): Predicted probabilities\n",
    "    \n",
    "    Returns:\n",
    "        float: Optimal threshold\n",
    "    \"\"\"\n",
    "    thresholds = np.linspace(0, 1, 100)\n",
    "    f1_scores = []\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        binary_actual = (actual >= np.median(actual)).astype(int)\n",
    "        binary_pred = (predicted >= threshold).astype(int)\n",
    "        \n",
    "        # Compute confusion matrix\n",
    "        tn, fp, fn, tp = confusion_matrix(binary_actual, binary_pred).ravel()\n",
    "        \n",
    "        # Compute F1 score\n",
    "        if tp + fp == 0 or tp + fn == 0:\n",
    "            f1 = 0\n",
    "        else:\n",
    "            precision = tp / (tp + fp)\n",
    "            recall = tp / (tp + fn)\n",
    "            f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "        \n",
    "        f1_scores.append(f1)\n",
    "    \n",
    "    # Return threshold with max F1 score\n",
    "    return thresholds[np.argmax(f1_scores)]\n",
    "\n",
    "def get_binary_metrics(actual, predicted, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Compute binary classification metrics for continuous labels\n",
    "    \n",
    "    Args:\n",
    "        actual (np.array): True labels\n",
    "        predicted (np.array): Predicted probabilities\n",
    "        threshold (float, optional): Classification threshold. If None, optimal threshold is computed.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Metrics including accuracy, precision, recall, F1, etc.\n",
    "    \"\"\"\n",
    "    # If no threshold provided, find optimal threshold\n",
    "    if threshold is None:\n",
    "        threshold = choose_optimal_threshold(actual, predicted)\n",
    "    \n",
    "    # Convert to binary classification\n",
    "    binary_actual = (actual >= np.median(actual)).astype(int)\n",
    "    binary_pred = (predicted >= threshold).astype(int)\n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(binary_actual, binary_pred).ravel()\n",
    "    \n",
    "    # Compute metrics\n",
    "    metrics = {\n",
    "        'threshold': threshold,\n",
    "        'accuracy': (tp + tn) / (tp + tn + fp + fn),\n",
    "        'precision': tp / (tp + fp) if tp + fp > 0 else 0,\n",
    "        'recall': tp / (tp + fn) if tp + fn > 0 else 0,\n",
    "        'specificity': tn / (tn + fp) if tn + fp > 0 else 0,\n",
    "        'f1_score': 2 * tp / (2 * tp + fp + fn) if tp > 0 else 0,\n",
    "        'auroc': roc_auc_score(binary_actual, predicted),\n",
    "        'auprc': average_precision_score(binary_actual, predicted)\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Modify existing metrics to use the new approach\n",
    "def get_mse(actual, predicted):\n",
    "    return ((actual - predicted) ** 2).mean()\n",
    "\n",
    "def get_accuracy(actual, predicted, threshold=None):\n",
    "    metrics = get_binary_metrics(actual, predicted, threshold)\n",
    "    return metrics['accuracy'] * 100.0\n",
    "\n",
    "def precision(actual, predicted, threshold=None):\n",
    "    metrics = get_binary_metrics(actual, predicted, threshold)\n",
    "    return metrics['precision']\n",
    "\n",
    "def sensitivity(actual, predicted, threshold=None):\n",
    "    metrics = get_binary_metrics(actual, predicted, threshold)\n",
    "    return metrics['recall']\n",
    "\n",
    "def specificity(actual, predicted, threshold=None):\n",
    "    metrics = get_binary_metrics(actual, predicted, threshold)\n",
    "    return metrics['specificity']\n",
    "\n",
    "def f_score(actual, predicted, threshold=None):\n",
    "    metrics = get_binary_metrics(actual, predicted, threshold)\n",
    "    return metrics['f1_score']\n",
    "\n",
    "def mcc(actual, predicted, threshold=None):\n",
    "    metrics = get_binary_metrics(actual, predicted, threshold)\n",
    "    \n",
    "    # Matthews Correlation Coefficient calculation\n",
    "    tp = metrics['precision'] * len(actual)\n",
    "    tn = metrics['specificity'] * len(actual)\n",
    "    fp = tp / metrics['precision'] - tp\n",
    "    fn = tp / metrics['recall'] - tp\n",
    "    \n",
    "    numerator = (tp * tn) - (fp * fn)\n",
    "    denominator = np.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
    "    \n",
    "    return numerator / denominator if denominator != 0 else 0\n",
    "\n",
    "def auroc(actual, predicted):\n",
    "    return get_binary_metrics(actual, predicted)['auroc']\n",
    "\n",
    "def auprc(actual, predicted):\n",
    "    return get_binary_metrics(actual, predicted)['auprc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97174cca-378a-482c-bf20-6a722b76a12a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : 0.015582451499309272\n",
      "Accuracy : 30.38943690916564\n",
      "precision: 0.93268416596105\n",
      "Sensititvity :0.14128134419290708\n",
      "specificity : 0.9589253422888142\n",
      "f-score : 0.24539125591757172\n",
      "MCC : 0.07652147866671151\n",
      "AUROC: 0.7501692704582001\n",
      "AUPRC: 0.9024788435946378\n"
     ]
    }
   ],
   "source": [
    "## model 5 epochs \n",
    "loss = get_mse(labels, predictions)\n",
    "acc = get_accuracy(labels, predictions, 0.5)\n",
    "prec = precision(labels, predictions, 0.5)\n",
    "sensitivity = sensitivity(labels, predictions,  0.5)\n",
    "specificity = specificity(labels, predictions, 0.5)\n",
    "f1 = f_score(labels, predictions, 0.5)\n",
    "mcc = mcc(labels, predictions,  0.5)\n",
    "auroc = auroc(labels, predictions)\n",
    "auprc = auprc(labels, predictions)\n",
    "\n",
    "\n",
    "print(f'loss : {loss}')\n",
    "print(f'Accuracy : {acc}')\n",
    "print(f'precision: {prec}')\n",
    "print(f'Sensititvity :{sensitivity}')\n",
    "print(f'specificity : {specificity}')\n",
    "print(f'f-score : {f1}')\n",
    "print(f'MCC : {mcc}')\n",
    "print(f'AUROC: {auroc}')\n",
    "print(f'AUPRC: {auprc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd2b98b4-7684-4431-8d5f-3f7f6da26d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## continue-valued metrics \n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "def get_rmse(actual, predicted):\n",
    "    \"\"\"\n",
    "    Root Mean Squared Error\n",
    "    \n",
    "    Args:\n",
    "        actual (np.array): True values\n",
    "        predicted (np.array): Predicted values\n",
    "    \n",
    "    Returns:\n",
    "        float: Root Mean Squared Error\n",
    "    \"\"\"\n",
    "    return np.sqrt(mean_squared_error(actual, predicted))\n",
    "\n",
    "def get_mae(actual, predicted):\n",
    "    \"\"\"\n",
    "    Mean Absolute Error\n",
    "    \n",
    "    Args:\n",
    "        actual (np.array): True values\n",
    "        predicted (np.array): Predicted values\n",
    "    \n",
    "    Returns:\n",
    "        float: Mean Absolute Error\n",
    "    \"\"\"\n",
    "    return mean_absolute_error(actual, predicted)\n",
    "\n",
    "def get_r2_score(actual, predicted):\n",
    "    \"\"\"\n",
    "    R-squared (Coefficient of Determination)\n",
    "    \n",
    "    Args:\n",
    "        actual (np.array): True values\n",
    "        predicted (np.array): Predicted values\n",
    "    \n",
    "    Returns:\n",
    "        float: R-squared score\n",
    "    \"\"\"\n",
    "    return r2_score(actual, predicted)\n",
    "\n",
    "def pearson_correlation(actual, predicted):\n",
    "    \"\"\"\n",
    "    Pearson Correlation Coefficient\n",
    "    \n",
    "    Args:\n",
    "        actual (np.array): True values\n",
    "        predicted (np.array): Predicted values\n",
    "    \n",
    "    Returns:\n",
    "        float: Pearson correlation coefficient\n",
    "    \"\"\"\n",
    "    return np.corrcoef(actual, predicted)[0, 1]\n",
    "\n",
    "def spearman_correlation(actual, predicted):\n",
    "    \"\"\"\n",
    "    Spearman Rank Correlation Coefficient\n",
    "    \n",
    "    Args:\n",
    "        actual (np.array): True values\n",
    "        predicted (np.array): Predicted values\n",
    "    \n",
    "    Returns:\n",
    "        float: Spearman correlation coefficient\n",
    "    \"\"\"\n",
    "    from scipy import stats\n",
    "    return stats.spearmanr(actual, predicted)[0]\n",
    "\n",
    "# Optional: Plotting function to visualize predictions vs actual values\n",
    "def plot_prediction_vs_actual(actual, predicted, title='Predictions vs Actual 5 epochs'):\n",
    "    \"\"\"\n",
    "    Create a scatter plot of predictions vs actual values\n",
    "    \n",
    "    Args:\n",
    "        actual (np.array): True values\n",
    "        predicted (np.array): Predicted values\n",
    "        title (str, optional): Plot title\n",
    "    \n",
    "    Returns:\n",
    "        matplotlib.figure.Figure: Matplotlib figure\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(actual, predicted, alpha=0.5)\n",
    "    plt.plot([actual.min(), actual.max()], [actual.min(), actual.max()], 'r--', lw=2)\n",
    "    plt.xlabel('Actual Values')\n",
    "    plt.ylabel('Predicted Values')\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return plt.gcf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "604a9f7e-3a8e-446b-b6cb-05184402365a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.12482968997521893\n",
      "MAE: 0.07864372484931197\n",
      "R-squared: 0.46821213455548116\n",
      "Pearson Correlation: 0.6854294136296294\n",
      "Spearman Correlation: 0.5431617919895632\n"
     ]
    }
   ],
   "source": [
    "# Existing code remains the same\n",
    "rmse = get_rmse(labels, predictions)\n",
    "mae = get_mae(labels, predictions)\n",
    "r2 = get_r2_score(labels, predictions)\n",
    "pearson = pearson_correlation(labels, predictions)\n",
    "spearman = spearman_correlation(labels, predictions)\n",
    "\n",
    "print(f'RMSE: {rmse}')\n",
    "print(f'MAE: {mae}')\n",
    "print(f'R-squared: {r2}')\n",
    "print(f'Pearson Correlation: {pearson}')\n",
    "print(f'Spearman Correlation: {spearman}')\n",
    "\n",
    "# Optional: Create a visualization\n",
    "fig = plot_prediction_vs_actual(labels, predictions)\n",
    "fig.savefig('predictions_vs_actual_5epochs.png')\n",
    "plt.close(fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
