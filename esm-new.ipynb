{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdc0a3a8-67ce-413c-b23f-83dde869dc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import glob\n",
    "import numpy as np \n",
    "import random\n",
    "import math\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from torch.utils.data import Dataset as Dataset_n\n",
    "from torch_geometric.data import DataLoader as DataLoader_n\n",
    "from torch_geometric.data import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d33fbd16-8492-458a-bc79-e1d3a0c3c3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bump(g):\n",
    "    return g\n",
    "    # return Data.from_dict(g.__dict__)\n",
    "\n",
    "class LabelledDataset(Dataset_n):\n",
    "    def __init__(self, npy_file, processed_dir):\n",
    "      self.npy_ar = np.load(npy_file)\n",
    "      self.processed_dir = processed_dir\n",
    "      self.protein_1 = self.npy_ar[:,2]\n",
    "      self.protein_2 = self.npy_ar[:,5]\n",
    "      self.label = self.npy_ar[:,6].astype(float)\n",
    "      self.n_samples = self.npy_ar.shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "      return(self.n_samples)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "      prot_1 = os.path.join(self.processed_dir, self.protein_1[index]+\".pt\")\n",
    "      prot_2 = os.path.join(self.processed_dir, self.protein_2[index]+\".pt\")\n",
    "      prot_1 = torch.load(glob.glob(prot_1)[0])['mean_representations'][33]\n",
    "      prot_2 = torch.load(glob.glob(prot_2)[0])['mean_representations'][33]\n",
    "      prot_1 = bump(prot_1)\n",
    "      prot_2 = bump(prot_2)\n",
    "      return prot_1, prot_2, torch.tensor(self.label[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37268963-2558-4909-a32e-2ee88e7b9f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "npy_file = \"../human_features/npy_file_new(human_dataset).npy\"\n",
    "npy_file_test = \"../human_features/npy_file_test.npy\"\n",
    "processed_dir=\"../human_features/esm/\"\n",
    "# train = LabelledDataset(npy_file=npy_file, processed_dir=processed_dir)\n",
    "# test = LabelledDataset(npy_file=npy_file_test, processed_dir=processed_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a2868f1-d807-4325-acfd-243ee88e7fd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe3c637b2dbb4af59d8056c2577319d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/97316 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TRAINING set  \n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "mapping_train = np.load(npy_file)\n",
    "bad_rows = []\n",
    "for row in tqdm(mapping_train):\n",
    "    try:\n",
    "        _, _, prot1, _, _, prot2, label = row\n",
    "        prot_1 = os.path.join(processed_dir, prot1+\".pt\")\n",
    "        prot_2 = os.path.join(processed_dir, prot2+\".pt\")\n",
    "        prot_1 = torch.load(glob.glob(prot_1)[0])['mean_representations'][33]\n",
    "        prot_2 = torch.load(glob.glob(prot_2)[0])['mean_representations'][33]\n",
    "        p = torch.cat([prot_1, prot_2])\n",
    "        label = float(label)\n",
    "        X_train.append(p)\n",
    "        y_train.append(label)\n",
    "    except:\n",
    "        bad_rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa2d73de-7348-4026-8cec-f0994ceeb0b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97014"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc8a180a-f302-4c76-b771-f33a08c6a839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97014"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fc09fc6e-61b0-4d31-a1ee-e4c24a95a2ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "461fcc032ab54992b1dcd53294159d23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5922 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TESTING set \n",
    "X_test = []\n",
    "y_test = []\n",
    "bad_test_rows = []\n",
    "mapping_test = np.load(npy_file_test)\n",
    "for row in tqdm(mapping_test):\n",
    "    try:\n",
    "        _, _, prot1, _, _, prot2, label = row\n",
    "        prot_1 = os.path.join(processed_dir, prot1+\".pt\")\n",
    "        prot_2 = os.path.join(processed_dir, prot2+\".pt\")\n",
    "        prot_1 = torch.load(glob.glob(prot_1)[0])['mean_representations'][33]\n",
    "        prot_2 = torch.load(glob.glob(prot_2)[0])['mean_representations'][33]\n",
    "        p = torch.cat([prot_1, prot_2])\n",
    "        label = float(label)\n",
    "        X_test.append(p)\n",
    "        y_test.append(label)\n",
    "    except:\n",
    "        bad_test_rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e927045-258b-45e7-bd6c-d5b7e5238c00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5907"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46600158-fc00-434c-9f3f-4dd1e25b5746",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5907"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "616cebf0-8da1-4d50-bcfe-2da9aecd8e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(X_train)):\n",
    "    X_train[i] = X_train[i][None, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3d7e7c56-b2de-471a-ae58-47c4c5740dca",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cat(): argument 'tensors' (position 1) must be tuple of Tensors, not Tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m X_train \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: cat(): argument 'tensors' (position 1) must be tuple of Tensors, not Tensor"
     ]
    }
   ],
   "source": [
    "X_train = torch.cat(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2e7cc9e8-24d1-4e1e-9329-e059350763da",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -qq scikit-learn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5965ecfe-05d6-4ab9-b11d-19239b7e6d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, val, ytrain, yval = train_test_split(X_train, y_train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d08ded45-8edd-4a99-b34f-86130b817d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain = [x for x in ytrain]\n",
    "yval = [x for x in yval]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6390d3e6-285c-4e66-a7a9-f004a4dea80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7603/3477652198.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train_torch = torch.tensor(X_train, dtype=torch.float32)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "619a8f3b5812482fa2b39e24d87872ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example predictions: tensor([0.3929, 0.3369, 0.3502, 0.4568, 0.3209, 0.2964, 0.2664, 0.2765, 0.3402,\n",
      "        0.3889])\n"
     ]
    }
   ],
   "source": [
    "# TRAINING \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "X_train_torch = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_torch = torch.tensor(y_train, dtype=torch.float32)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(in_features=X_train.shape[1], out_features=1)\n",
    ")\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "dataset = TensorDataset(X_train_torch, y_train_torch)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    for X_batch, y_batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(X_batch).squeeze()\n",
    "        loss = criterion(logits, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(X_train_torch).squeeze(dim=1)\n",
    "    probs = torch.sigmoid(logits)\n",
    "    print(\"Example predictions:\", probs[:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
