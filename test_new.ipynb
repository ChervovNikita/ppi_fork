{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b15fb161-108b-4129-ba9f-2801258f5695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len test dataset: 5922\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import glob\n",
    "import numpy as np \n",
    "import random\n",
    "import math\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from torch.utils.data import Dataset as Dataset_n\n",
    "from torch_geometric.data import DataLoader as DataLoader_n\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from metrics import *\n",
    "from models import GCNN, AttGNN\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "npy_file = \"../human_features/npy_file_test.npy\"\n",
    "processed_dir=\"../human_features/processed/\"\n",
    "def bump(g):\n",
    "    return g\n",
    "    # return Data.from_dict(g.__dict__)\n",
    "\n",
    "class LabelledDataset(Dataset_n):\n",
    "    def __init__(self, npy_file, processed_dir):\n",
    "      self.npy_ar = np.load(npy_file)\n",
    "      self.processed_dir = processed_dir\n",
    "      self.protein_1 = self.npy_ar[:,2]\n",
    "      self.protein_2 = self.npy_ar[:,5]\n",
    "      self.label = self.npy_ar[:,6].astype(float)\n",
    "      self.n_samples = self.npy_ar.shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "      return(self.n_samples)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "      prot_1 = os.path.join(self.processed_dir, self.protein_1[index]+\".pt\")\n",
    "      prot_2 = os.path.join(self.processed_dir, self.protein_2[index]+\".pt\")\n",
    "      # print(prot_1, prot_2)\n",
    "      # print(glob.glob(prot_1), glob.glob(prot_2))\n",
    "      #print(f'Second prot is {prot_2}')\n",
    "      prot_1 = torch.load(glob.glob(prot_1)[0])\n",
    "      #print(f'Here lies {glob.glob(prot_2)}')\n",
    "      prot_2 = torch.load(glob.glob(prot_2)[0])\n",
    "      prot_1 = bump(prot_1)\n",
    "      prot_2 = bump(prot_2)\n",
    "      prot_1.x = prot_1.x.to(torch.float32)\n",
    "      prot_2.x = prot_2.x.to(torch.float32)\n",
    "      return prot_1, prot_2, torch.tensor(self.label[index])\n",
    "\n",
    "dataset = LabelledDataset(npy_file = npy_file ,processed_dir= processed_dir)\n",
    "print(f'len test dataset: {len(dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ba06183-c762-4d0a-90c3-146b8f5cec8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1481"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testloader = DataLoader_n(dataset=dataset, batch_size=4, num_workers=0)\n",
    "len(testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fee5011-8c7d-4b7f-b856-949bc0e97094",
   "metadata": {},
   "outputs": [],
   "source": [
    "## test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fddc509-c0a7-43ed-ba87-90f4cdce378c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCNN Loaded\n",
      "GCNN(\n",
      "  (pro1_conv1): GCNConv(1024, 1024)\n",
      "  (pro1_fc1): Linear(in_features=1024, out_features=128, bias=True)\n",
      "  (pro2_conv1): GCNConv(1024, 1024)\n",
      "  (pro2_fc1): Linear(in_features=1024, out_features=128, bias=True)\n",
      "  (relu): LeakyReLU(negative_slope=0.01)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (sigmoid): Sigmoid()\n",
      "  (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "  (out): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "AttGNN Loaded\n",
      "AttGNN(\n",
      "  (pro1_conv1): GATConv(1024, 128, heads=1)\n",
      "  (pro1_fc1): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (pro2_conv1): GATConv(1024, 128, heads=1)\n",
      "  (pro2_fc1): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (relu): LeakyReLU(negative_slope=0.01)\n",
      "  (sigmoid): Sigmoid()\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "  (out): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "GCNN Loaded\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b1ca884ef6f40c38d838065dbf5b30a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1481 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.cuda(\"cpu\")\n",
    "model = GCNN()\n",
    "model.load_state_dict(torch.load(\"../human_features/GCN_50.pth\")) \n",
    "model.to(device)\n",
    "model.eval()\n",
    "predictions = torch.Tensor()\n",
    "labels = torch.Tensor()\n",
    "with torch.no_grad():\n",
    "    for prot_1, prot_2, label in tqdm(testloader):\n",
    "      prot_1 = prot_1.to(device)\n",
    "      prot_2 = prot_2.to(device)\n",
    "      #print(\"H\")\n",
    "      #print(torch.Tensor.size(prot_1.x), torch.Tensor.size(prot_2.x))\n",
    "      output = model(prot_1, prot_2)\n",
    "      predictions = torch.cat((predictions, output.cpu()), 0)\n",
    "      labels = torch.cat((labels, label.view(-1,1).cpu()), 0)\n",
    "labels = labels.numpy().flatten()\n",
    "predictions = predictions.numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03aafb06-39b1-4857-b547-5b9f7de61ebc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels unique values: [0.       0.254085 0.312347 0.335467 0.402325 0.448666 0.546693 0.555837\n",
      " 0.564025 0.590789 0.599047 0.649687 0.657212 0.688877 0.731766 0.781149\n",
      " 0.803914 0.841009 0.841369 0.84589  0.850172 0.858674 0.863618 0.890524\n",
      " 0.893722 0.903702 0.922873 0.929143 0.936921 0.93774  0.946618 0.95106\n",
      " 0.9573   0.958786 0.962619 0.968646 0.977758 0.982325 0.984882 0.988004\n",
      " 0.988808 0.990568 0.993616]\n",
      "Labels distribution: [5922]\n",
      "Predictions range: 5.7578063e-07 0.6520468\n",
      "Predictions distribution:\n",
      "(array([  88,   29,   39,   51,  550, 3471,  840,  641,  189,   24]), array([5.7578063e-07, 6.5205202e-02, 1.3040982e-01, 1.9561444e-01,\n",
      "       2.6081908e-01, 3.2602370e-01, 3.9122832e-01, 4.5643294e-01,\n",
      "       5.2163756e-01, 5.8684218e-01, 6.5204680e-01], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "print(\"Labels unique values:\", np.unique(labels))\n",
    "print(\"Labels distribution:\", np.bincount(labels.astype(int)))\n",
    "print(\"Predictions range:\", predictions.min(), predictions.max())\n",
    "print(\"Predictions distribution:\")\n",
    "print(np.histogram(predictions, bins=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "635975a6-5413-4517-a29c-0e967d60424d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error calculating metrics: division by zero\n",
      "Detailed diagnostic information:\n",
      "Labels: [0.335467 0.335467 0.335467 ... 0.335467 0.335467 0.335467]\n",
      "Predictions: [0.52831584 0.36795896 0.36130196 ... 0.38861328 0.35173368 0.37437233]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    loss = get_mse(labels, predictions)\n",
    "    acc = get_accuracy(labels, predictions, 0.5)\n",
    "    prec = precision(labels, predictions, 0.5)\n",
    "    sensitivity = sensitivity(labels, predictions,  0.5)\n",
    "    specificity = specificity(labels, predictions, 0.5)\n",
    "    f1 = f_score(labels, predictions, 0.5)\n",
    "    mcc = mcc(labels, predictions,  0.5)\n",
    "    auroc = auroc(labels, predictions)\n",
    "    auprc = auprc(labels, predictions)\n",
    "except Exception as e:\n",
    "    print(f\"Error calculating metrics: {e}\")\n",
    "    print(\"Detailed diagnostic information:\")\n",
    "    print(\"Labels:\", labels)\n",
    "    print(\"Predictions:\", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "425ec4b9-c888-437e-a086-40c41977a659",
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert continue valued metrics to binary metrics \n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, confusion_matrix\n",
    "\n",
    "def choose_optimal_threshold(actual, predicted):\n",
    "    \"\"\"\n",
    "    Choose the optimal threshold that maximizes the F1 score\n",
    "    \n",
    "    Args:\n",
    "        actual (np.array): True labels\n",
    "        predicted (np.array): Predicted probabilities\n",
    "    \n",
    "    Returns:\n",
    "        float: Optimal threshold\n",
    "    \"\"\"\n",
    "    thresholds = np.linspace(0, 1, 100)\n",
    "    f1_scores = []\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        binary_actual = (actual >= np.median(actual)).astype(int)\n",
    "        binary_pred = (predicted >= threshold).astype(int)\n",
    "        \n",
    "        # Compute confusion matrix\n",
    "        tn, fp, fn, tp = confusion_matrix(binary_actual, binary_pred).ravel()\n",
    "        \n",
    "        # Compute F1 score\n",
    "        if tp + fp == 0 or tp + fn == 0:\n",
    "            f1 = 0\n",
    "        else:\n",
    "            precision = tp / (tp + fp)\n",
    "            recall = tp / (tp + fn)\n",
    "            f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "        \n",
    "        f1_scores.append(f1)\n",
    "    \n",
    "    # Return threshold with max F1 score\n",
    "    return thresholds[np.argmax(f1_scores)]\n",
    "\n",
    "def get_binary_metrics(actual, predicted, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Compute binary classification metrics for continuous labels\n",
    "    \n",
    "    Args:\n",
    "        actual (np.array): True labels\n",
    "        predicted (np.array): Predicted probabilities\n",
    "        threshold (float, optional): Classification threshold. If None, optimal threshold is computed.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Metrics including accuracy, precision, recall, F1, etc.\n",
    "    \"\"\"\n",
    "    # If no threshold provided, find optimal threshold\n",
    "    if threshold is None:\n",
    "        threshold = choose_optimal_threshold(actual, predicted)\n",
    "    \n",
    "    # Convert to binary classification\n",
    "    binary_actual = (actual >= np.median(actual)).astype(int)\n",
    "    binary_pred = (predicted >= threshold).astype(int)\n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(binary_actual, binary_pred).ravel()\n",
    "    \n",
    "    # Compute metrics\n",
    "    metrics = {\n",
    "        'threshold': threshold,\n",
    "        'accuracy': (tp + tn) / (tp + tn + fp + fn),\n",
    "        'precision': tp / (tp + fp) if tp + fp > 0 else 0,\n",
    "        'recall': tp / (tp + fn) if tp + fn > 0 else 0,\n",
    "        'specificity': tn / (tn + fp) if tn + fp > 0 else 0,\n",
    "        'f1_score': 2 * tp / (2 * tp + fp + fn) if tp > 0 else 0,\n",
    "        'auroc': roc_auc_score(binary_actual, predicted),\n",
    "        'auprc': average_precision_score(binary_actual, predicted)\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Modify existing metrics to use the new approach\n",
    "def get_mse(actual, predicted):\n",
    "    return ((actual - predicted) ** 2).mean()\n",
    "\n",
    "def get_accuracy(actual, predicted, threshold=None):\n",
    "    metrics = get_binary_metrics(actual, predicted, threshold)\n",
    "    return metrics['accuracy'] * 100.0\n",
    "\n",
    "def precision(actual, predicted, threshold=None):\n",
    "    metrics = get_binary_metrics(actual, predicted, threshold)\n",
    "    return metrics['precision']\n",
    "\n",
    "def sensitivity(actual, predicted, threshold=None):\n",
    "    metrics = get_binary_metrics(actual, predicted, threshold)\n",
    "    return metrics['recall']\n",
    "\n",
    "def specificity(actual, predicted, threshold=None):\n",
    "    metrics = get_binary_metrics(actual, predicted, threshold)\n",
    "    return metrics['specificity']\n",
    "\n",
    "def f_score(actual, predicted, threshold=None):\n",
    "    metrics = get_binary_metrics(actual, predicted, threshold)\n",
    "    return metrics['f1_score']\n",
    "\n",
    "def mcc(actual, predicted, threshold=None):\n",
    "    metrics = get_binary_metrics(actual, predicted, threshold)\n",
    "    \n",
    "    # Matthews Correlation Coefficient calculation\n",
    "    tp = metrics['precision'] * len(actual)\n",
    "    tn = metrics['specificity'] * len(actual)\n",
    "    fp = tp / metrics['precision'] - tp\n",
    "    fn = tp / metrics['recall'] - tp\n",
    "    \n",
    "    numerator = (tp * tn) - (fp * fn)\n",
    "    denominator = np.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
    "    \n",
    "    return numerator / denominator if denominator != 0 else 0\n",
    "\n",
    "def auroc(actual, predicted):\n",
    "    return get_binary_metrics(actual, predicted)['auroc']\n",
    "\n",
    "def auprc(actual, predicted):\n",
    "    return get_binary_metrics(actual, predicted)['auprc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "79d06da0-5d64-4fd9-a9e2-0bdaa9105218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : 0.030255012972890634\n",
      "Accuracy : 25.34616683552854\n",
      "precision: 0.8567493112947658\n",
      "Sensititvity :0.06645299145299145\n",
      "specificity : 0.9581320450885669\n",
      "f-score : 0.12333928217330954\n",
      "MCC : -0.066497187510526\n",
      "AUROC: 0.6020689678902239\n",
      "AUPRC: 0.830453907547192\n"
     ]
    }
   ],
   "source": [
    "loss = get_mse(labels, predictions)\n",
    "acc = get_accuracy(labels, predictions, 0.5)\n",
    "prec = precision(labels, predictions, 0.5)\n",
    "sensitivity = sensitivity(labels, predictions,  0.5)\n",
    "specificity = specificity(labels, predictions, 0.5)\n",
    "f1 = f_score(labels, predictions, 0.5)\n",
    "mcc = mcc(labels, predictions,  0.5)\n",
    "auroc = auroc(labels, predictions)\n",
    "auprc = auprc(labels, predictions)\n",
    "\n",
    "\n",
    "print(f'loss : {loss}')\n",
    "print(f'Accuracy : {acc}')\n",
    "print(f'precision: {prec}')\n",
    "print(f'Sensititvity :{sensitivity}')\n",
    "print(f'specificity : {specificity}')\n",
    "print(f'f-score : {f1}')\n",
    "print(f'MCC : {mcc}')\n",
    "print(f'AUROC: {auroc}')\n",
    "print(f'AUPRC: {auprc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76113bc6-8105-45fc-8cf1-ec1c7d27df09",
   "metadata": {},
   "outputs": [],
   "source": [
    "## continue-valued metrics \n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "def get_rmse(actual, predicted):\n",
    "    \"\"\"\n",
    "    Root Mean Squared Error\n",
    "    \n",
    "    Args:\n",
    "        actual (np.array): True values\n",
    "        predicted (np.array): Predicted values\n",
    "    \n",
    "    Returns:\n",
    "        float: Root Mean Squared Error\n",
    "    \"\"\"\n",
    "    return np.sqrt(mean_squared_error(actual, predicted))\n",
    "\n",
    "def get_mae(actual, predicted):\n",
    "    \"\"\"\n",
    "    Mean Absolute Error\n",
    "    \n",
    "    Args:\n",
    "        actual (np.array): True values\n",
    "        predicted (np.array): Predicted values\n",
    "    \n",
    "    Returns:\n",
    "        float: Mean Absolute Error\n",
    "    \"\"\"\n",
    "    return mean_absolute_error(actual, predicted)\n",
    "\n",
    "def get_r2_score(actual, predicted):\n",
    "    \"\"\"\n",
    "    R-squared (Coefficient of Determination)\n",
    "    \n",
    "    Args:\n",
    "        actual (np.array): True values\n",
    "        predicted (np.array): Predicted values\n",
    "    \n",
    "    Returns:\n",
    "        float: R-squared score\n",
    "    \"\"\"\n",
    "    return r2_score(actual, predicted)\n",
    "\n",
    "def pearson_correlation(actual, predicted):\n",
    "    \"\"\"\n",
    "    Pearson Correlation Coefficient\n",
    "    \n",
    "    Args:\n",
    "        actual (np.array): True values\n",
    "        predicted (np.array): Predicted values\n",
    "    \n",
    "    Returns:\n",
    "        float: Pearson correlation coefficient\n",
    "    \"\"\"\n",
    "    return np.corrcoef(actual, predicted)[0, 1]\n",
    "\n",
    "def spearman_correlation(actual, predicted):\n",
    "    \"\"\"\n",
    "    Spearman Rank Correlation Coefficient\n",
    "    \n",
    "    Args:\n",
    "        actual (np.array): True values\n",
    "        predicted (np.array): Predicted values\n",
    "    \n",
    "    Returns:\n",
    "        float: Spearman correlation coefficient\n",
    "    \"\"\"\n",
    "    from scipy import stats\n",
    "    return stats.spearmanr(actual, predicted)[0]\n",
    "\n",
    "# Optional: Plotting function to visualize predictions vs actual values\n",
    "def plot_prediction_vs_actual(actual, predicted, title='Predictions vs Actual'):\n",
    "    \"\"\"\n",
    "    Create a scatter plot of predictions vs actual values\n",
    "    \n",
    "    Args:\n",
    "        actual (np.array): True values\n",
    "        predicted (np.array): Predicted values\n",
    "        title (str, optional): Plot title\n",
    "    \n",
    "    Returns:\n",
    "        matplotlib.figure.Figure: Matplotlib figure\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(actual, predicted, alpha=0.5)\n",
    "    plt.plot([actual.min(), actual.max()], [actual.min(), actual.max()], 'r--', lw=2)\n",
    "    plt.xlabel('Actual Values')\n",
    "    plt.ylabel('Predicted Values')\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return plt.gcf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "20434519-56cd-4cf9-a7aa-96c27ebd1180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.17393968199606044\n",
      "MAE: 0.11947577226948514\n",
      "R-squared: 0.07805297426553204\n",
      "Pearson Correlation: 0.31022231981260273\n",
      "Spearman Correlation: 0.23707677788774872\n"
     ]
    }
   ],
   "source": [
    "# Existing code remains the same\n",
    "rmse = get_rmse(labels, predictions)\n",
    "mae = get_mae(labels, predictions)\n",
    "r2 = get_r2_score(labels, predictions)\n",
    "pearson = pearson_correlation(labels, predictions)\n",
    "spearman = spearman_correlation(labels, predictions)\n",
    "\n",
    "print(f'RMSE: {rmse}')\n",
    "print(f'MAE: {mae}')\n",
    "print(f'R-squared: {r2}')\n",
    "print(f'Pearson Correlation: {pearson}')\n",
    "print(f'Spearman Correlation: {spearman}')\n",
    "\n",
    "# Optional: Create a visualization\n",
    "fig = plot_prediction_vs_actual(labels, predictions)\n",
    "fig.savefig('predictions_vs_actual_5epochs.png')\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0814a368-f465-44f9-b460-c2c8e227e98f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.030255012972890634"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse = get_mse(labels, predictions)\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c1fc5cc2-8312-462f-9a56-712f774f8680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MSE Diagnostic Information:\n",
      "Labels mean: 0.3667525682201959\n",
      "Labels standard deviation: 0.18115306563235456\n",
      "Predictions mean: 0.3716069459915161\n",
      "Predictions standard deviation: 0.08013929426670074\n",
      "Baseline MSE (predicting mean): 0.032816433188000156\n",
      "Current Model MSE: 0.030255012972890634\n",
      "Improvement over baseline: 7.81%\n"
     ]
    }
   ],
   "source": [
    "# Add MSE diagnostic information\n",
    "print(\"\\nMSE Diagnostic Information:\")\n",
    "print(f\"Labels mean: {np.mean(labels)}\")\n",
    "print(f\"Labels standard deviation: {np.std(labels)}\")\n",
    "print(f\"Predictions mean: {np.mean(predictions)}\")\n",
    "print(f\"Predictions standard deviation: {np.std(predictions)}\")\n",
    "\n",
    "# Compute baseline MSE (using mean prediction)\n",
    "baseline_mse = np.mean((labels - np.mean(labels))**2)\n",
    "print(f\"Baseline MSE (predicting mean): {baseline_mse}\")\n",
    "print(f\"Current Model MSE: {loss}\")\n",
    "print(f\"Improvement over baseline: {(baseline_mse - loss) / baseline_mse * 100:.2f}%\")\n",
    "\n",
    "# Visualize prediction errors\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(labels, predictions, alpha=0.5)\n",
    "plt.plot([labels.min(), labels.max()], [labels.min(), labels.max()], 'r--', lw=2)\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('Prediction Accuracy')\n",
    "plt.savefig('prediction_accuracy.png')\n",
    "plt.close()\n",
    "\n",
    "# Error distribution\n",
    "errors = labels - predictions\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(errors, bins=50)\n",
    "plt.xlabel('Prediction Errors')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Prediction Errors')\n",
    "plt.savefig('error_distribution.png')\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
