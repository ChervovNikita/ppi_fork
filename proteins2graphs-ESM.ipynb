{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e95e931-e193-4ee2-b8a7-c00249ad0f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import pathlib\n",
    "\n",
    "import biographs as bg\n",
    "from Bio import SeqIO\n",
    "from Bio.PDB.PDBParser import PDBParser\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import re\n",
    "import torch\n",
    "\n",
    "import torch\n",
    "import networkx as nx\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import add_self_loops, degree\n",
    "from torch_geometric.data import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94076b14-0363-41de-8477-3ced77674e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2572ce75-eeea-4041-b32a-2009a6186503",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6dca086f98245589803875a6e2670be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/95.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deafd9ef8a464d5fac85bfc5f5a0849c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/93.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65d02123c19545bba1abd738e216f50f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4258b6c1f6ca41379078b13d9c33b308",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/724 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0ec1c2f2aee4405b7328e6b20249fd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.61G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# ESM-2 model loading\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t33_650M_UR50D\")\n",
    "model = AutoModel.from_pretrained(\"facebook/esm2_t33_650M_UR50D\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2af591c8-5c50-4575-91b4-fb7e4c408293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of 20 proteins\n",
    "pro_res_table = [\n",
    "    \"A\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"K\", \"L\", \n",
    "    \"M\", \"N\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"V\", \"W\", \"Y\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33b604c2-337d-4e83-a0c2-da4ad32bf61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProteinDataset:\n",
    "    def __init__(self, root, transform=None, pre_transform=None):\n",
    "        self.root = root\n",
    "        self.processed_dir = os.path.join(root, \"processed_esm\")\n",
    "        os.makedirs(self.processed_dir, exist_ok=True)\n",
    "        self.raw_paths = [os.path.join(root, \"raw\", f) for f in os.listdir(os.path.join(root, \"raw\")) if f.endswith(\".pdb\")]\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return [filename.name for filename in os.scandir(self.root + \"/raw\")]\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return [\n",
    "            os.path.splitext(os.path.basename(file))[0] + \".pt\"\n",
    "            for file in self.raw_paths\n",
    "        ]\n",
    "\n",
    "    def _get_structure(self, file):\n",
    "        parser = PDBParser(QUIET=True)\n",
    "        structure = parser.get_structure('protein', file)\n",
    "        return structure\n",
    "\n",
    "    def _get_sequence(self, structure):\n",
    "        seq = \"\"\n",
    "        for model in structure:\n",
    "            for chain in model:\n",
    "                for residue in chain:\n",
    "                    if residue.get_resname() in bg.aminoacids3to1:\n",
    "                        seq += bg.aminoacids3to1[residue.get_resname()]\n",
    "        return seq\n",
    "        \n",
    "    def _get_esm_embeddings(self, sequence):\n",
    "        # Truncate sequence if too long\n",
    "        max_length = 1024  # ESM-2 max sequence length\n",
    "        sequence = sequence[:max_length]\n",
    "        \n",
    "        inputs = tokenizer(sequence, return_tensors=\"pt\", add_special_tokens=True).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        # Average pooling of token embeddings, excluding special tokens\n",
    "        embeddings = outputs.last_hidden_state.squeeze(0)[1:-1]  # Remove [CLS] and [SEP] tokens\n",
    "        return embeddings.to('cpu')  # Move back to CPU for compatibility\n",
    "    def _get_adjacency(self, file):\n",
    "        # Existing adjacency matrix generation logic\n",
    "        parser = PDBParser(QUIET=True)\n",
    "        structure = parser.get_structure('protein', file)\n",
    "        \n",
    "        # Create a simple distance-based adjacency matrix\n",
    "        atoms = list(structure.get_atoms())\n",
    "        n = len(atoms)\n",
    "        adjacency_mat = np.zeros((n, n))\n",
    "        \n",
    "        for i in range(n):\n",
    "            for j in range(i+1, n):\n",
    "                distance = np.linalg.norm(atoms[i].coord - atoms[j].coord)\n",
    "                if distance < 8.0:  # Threshold for considering an edge\n",
    "                    adjacency_mat[i, j] = adjacency_mat[j, i] = 1\n",
    "        \n",
    "        return torch.tensor(adjacency_mat, dtype=torch.float)\n",
    "\n",
    "    def _get_edgeindex(self, file, adjacency_mat):\n",
    "        # Convert adjacency matrix to edge index\n",
    "        edge_index = torch.nonzero(adjacency_mat).t().contiguous()\n",
    "        return edge_index\n",
    "\n",
    "    def process(self):\n",
    "        data_list = []\n",
    "        count = 0\n",
    "        for file in tqdm(self.raw_paths):\n",
    "            if pathlib.Path(file).suffix == \".pdb\":\n",
    "                try:\n",
    "                    struct = self._get_structure(file)\n",
    "                    seq = self._get_sequence(struct)\n",
    "\n",
    "                    # Node features extracted using ESM-2\n",
    "                    node_feats = self._get_esm_embeddings(seq)\n",
    "\n",
    "                    # Edge index extracted\n",
    "                    mat = self._get_adjacency(file)\n",
    "\n",
    "                    # Ensure node features and adjacency matrix are compatible\n",
    "                    if mat.shape[0] >= torch.Tensor.size(node_feats)[0]:\n",
    "                        edge_index = self._get_edgeindex(file, mat)\n",
    "\n",
    "                        # Create data object\n",
    "                        data = Data(x=node_feats, edge_index=edge_index)\n",
    "                        count += 1\n",
    "                        data_list.append(data)\n",
    "\n",
    "                        # Save processed graph\n",
    "                        torch.save(\n",
    "                            data,\n",
    "                            os.path.join(\n",
    "                                self.processed_dir,\n",
    "                                os.path.splitext(os.path.basename(file))[0] + \".pt\"\n",
    "                            )\n",
    "                        )\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {file}: {e}\")\n",
    "\n",
    "        self.data_prot = data_list\n",
    "        print(f\"Processed {count} protein graphs\")\n",
    "\n",
    "prot_graphs = ProteinDataset(\"../human_features/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9724c8c7-9f66-4b34-8789-db2fbc61f510",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
